# -*- coding: utf-8 -*-
"""Time series project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KADAxH-b6W7F7gImk7dYJKAjaUwJyZ14
"""

import pandas as pd

# 1. Prepare the `df` DataFrame for Prophet
# Create a copy to avoid modifying the original `df` directly
prophet_df = df.copy()

# Reset the index to create a 'ds' column from the 'date' index
prophet_df = prophet_df.reset_index()

# Rename the 'date' column to 'ds' and the target variable 'T' to 'y'
prophet_df = prophet_df.rename(columns={'date': 'ds', 'T': 'y'})

# Select the columns relevant for Prophet: 'ds', 'y', and the feature columns (regressors)
# Ensure all feature_cols are present in prophet_df before selection
prophet_cols = ['ds', 'y'] + [col for col in feature_cols if col in prophet_df.columns]
prophet_df = prophet_df[prophet_cols]

print("First 5 rows of the Prophet-ready DataFrame:")
print(prophet_df.head())
print(f"\nColumns in prophet_df: {prophet_df.columns.tolist()}")

"""**Reasoning**:
Now that the `prophet_df` is prepared, I will perform a temporal train-test split using the first 80% of the data for training and the remaining 20% for testing, ensuring the time series order is preserved as per instruction 2.


"""

from sklearn.model_selection import train_test_split

# Calculate the split point (80% for training, 20% for testing)
train_size_prophet = int(len(prophet_df) * 0.8)

# Split the data into training and testing sets, maintaining temporal order
prophet_train_df = prophet_df.iloc[:train_size_prophet]
prophet_test_df = prophet_df.iloc[train_size_prophet:]

print(f"Shape of Prophet training data: {prophet_train_df.shape}")
print(f"Shape of Prophet testing data: {prophet_test_df.shape}")
print("First 5 rows of Prophet training data:")
print(prophet_train_df.head())
print("First 5 rows of Prophet testing data:")
print(prophet_test_df.head())

"""**Reasoning**:
Now that the data is split into training and testing sets, I will initialize the Prophet model, add the defined regressors from `feature_cols` (excluding 'ds' and 'y'), and configure it with daily, weekly, and yearly seasonalities as suggested in the instructions to capture time-based patterns in the weather data.


"""

from prophet import Prophet

# Initialize Prophet model with daily, weekly, and yearly seasonality
model = Prophet(
    daily_seasonality=True,
    weekly_seasonality=True,
    yearly_seasonality=True,
    seasonality_mode='multiplicative' # Often works well with environmental data that has varying seasonal amplitude
)

# Add regressors (feature columns) to the model
# Exclude 'ds' and 'y' as they are already handled by Prophet
regressors = [col for col in feature_cols if col not in ['ds', 'y']]
for col in regressors:
    model.add_regressor(col)

print("Prophet model initialized with daily, weekly, and yearly seasonality and additional regressors.")
print(f"Regressors added: {regressors}")

"""**Reasoning**:
With the Prophet model initialized and regressors added, the next step is to fit the model to the training data (`prophet_train_df`), which is instruction 4 of the subtask.


"""

print("Fitting Prophet model...")
model.fit(prophet_train_df)
print("Prophet model fitted successfully.")

"""**Reasoning**:
Now that the Prophet model has been fitted, I will create a DataFrame for future predictions using `make_future_dataframe()`. This DataFrame will extend for the duration of the test set (`prophet_test_df`) and will also include the regressors from the test set, as per instruction 5.


"""

future = model.make_future_dataframe(periods=len(prophet_test_df), freq='10min', include_history=False)

# Prophet's make_future_dataframe only creates 'ds' column. We need to add the regressors for prediction.
# Align the regressors from the prophet_test_df to the future DataFrame based on 'ds'.
# First, ensure 'ds' in future is datetime.
future['ds'] = pd.to_datetime(future['ds'])

# Merge future with prophet_test_df to bring in the regressor values for the future dates
# We'll merge on the 'ds' column.
regressor_cols = [col for col in regressors if col in prophet_test_df.columns]
future = pd.merge(future, prophet_test_df[['ds'] + regressor_cols], on='ds', how='left')

print("First 5 rows of the future DataFrame with regressors:")
print(future.head())
print(f"Shape of future DataFrame: {future.shape}")

"""**Reasoning**:
Now that the `future` DataFrame is prepared with regressors, I will generate predictions using the fitted Prophet model on this DataFrame. This will produce forecasts including prediction intervals (yhat_lower and yhat_upper), which is instruction 6 of the subtask.


"""

print("Generating Prophet forecasts...")
forecast = model.predict(future)
print("Prophet forecasts generated successfully.")

# Display the first few rows of the forecast, focusing on relevant columns
print("\nFirst 5 rows of the Prophet forecast (yhat, yhat_lower, yhat_upper):")
print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].head())

"""**Reasoning**:
Now that the Prophet forecasts have been generated, I will evaluate the model's performance on the test set by calculating Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) between the forecasted 'yhat' and the actual 'y' values, as per instruction 7 of the subtask.


"""

from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Align the forecast with the actual test data
# prophet_test_df contains the actual 'y' values and 'ds' (datestamp)

# Ensure 'ds' column is datetime in both forecast and prophet_test_df for accurate merging
forecast['ds'] = pd.to_datetime(forecast['ds'])
prophet_test_df['ds'] = pd.to_datetime(prophet_test_df['ds'])

# Merge the actual values from prophet_test_df with the forecast
performance_df = pd.merge(prophet_test_df[['ds', 'y']], forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']], on='ds', how='inner')

# Calculate evaluation metrics
mae = mean_absolute_error(performance_df['y'], performance_df['yhat'])
rmse = np.sqrt(mean_squared_error(performance_df['y'], performance_df['yhat']))

print(f"\nProphet Model Performance on Test Set:")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")

print("\nFirst 5 rows of performance_df (actual vs. predicted with intervals):")
print(performance_df.head())

"""**Reasoning**:
The previous code produced a `SettingWithCopyWarning` because it was attempting to modify a slice of a DataFrame. To resolve this, I will explicitly create a copy of `prophet_test_df` before modifying its 'ds' column to ensure changes are made on an independent DataFrame, thereby avoiding the warning.


"""

from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Align the forecast with the actual test data
# prophet_test_df contains the actual 'y' values and 'ds' (datestamp)

# Create a copy of prophet_test_df to avoid SettingWithCopyWarning
prophet_test_df_copy = prophet_test_df.copy()

# Ensure 'ds' column is datetime in both forecast and prophet_test_df_copy for accurate merging
forecast['ds'] = pd.to_datetime(forecast['ds'])
prophet_test_df_copy['ds'] = pd.to_datetime(prophet_test_df_copy['ds'])

# Merge the actual values from prophet_test_df_copy with the forecast
performance_df = pd.merge(prophet_test_df_copy[['ds', 'y']], forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']], on='ds', how='inner')

# Calculate evaluation metrics
mae = mean_absolute_error(performance_df['y'], performance_df['yhat'])
rmse = np.sqrt(mean_squared_error(performance_df['y'], performance_df['yhat']))

print(f"\nProphet Model Performance on Test Set:")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")

print("\nFirst 5 rows of performance_df (actual vs. predicted with intervals):")
print(performance_df.head())

"""**Reasoning**:
Now that the Prophet model's performance has been evaluated, I will proceed to visualize the forecast, including the actual observed data and the prediction intervals, to analyze the characteristics of its uncertainty against the actual values as per instruction 8 of the subtask.


"""

import matplotlib.pyplot as plt
import seaborn as sns

# Plotting the forecast with actual values and uncertainty intervals
plt.figure(figsize=(14, 7))
sns.lineplot(x='ds', y='y', data=performance_df, label='Actual Values', color='blue')
sns.lineplot(x='ds', y='yhat', data=performance_df, label='Prophet Forecast', color='red')
plt.fill_between(performance_df['ds'], performance_df['yhat_lower'], performance_df['yhat_upper'], color='red', alpha=0.2, label='Prediction Interval (80%)')

plt.title('Prophet Forecast vs. Actual Values with Prediction Intervals')
plt.xlabel('Date')
plt.ylabel('Temperature (Scaled)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Optionally, zoom into a smaller time frame for better detail
plt.figure(figsize=(14, 7))
sns.lineplot(x='ds', y='y', data=performance_df.head(200), label='Actual Values', color='blue')
sns.lineplot(x='ds', y='yhat', data=performance_df.head(200), label='Prophet Forecast', color='red')
plt.fill_between(performance_df['ds'].head(200), performance_df['yhat_lower'].head(200), performance_df['yhat_upper'].head(200), color='red', alpha=0.2, label='Prediction Interval (80%)')

plt.title('Prophet Forecast vs. Actual Values (Zoomed) with Prediction Intervals')
plt.xlabel('Date')
plt.ylabel('Temperature (Scaled)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""**Reasoning**:
I will define the sequence length and then create the sequential data (X and y) using a sliding window approach from the preprocessed features and target variable. This involves iterating through the DataFrame to construct input sequences and their corresponding target values, then converting them into NumPy arrays.


"""

import numpy as np

# 1. Define a suitable sequence length
sequence_length = 24  # 24 steps for 4 hours of 10-minute data points

# Prepare the feature and target data
features = df[feature_cols]
target = df[target_variable]

# 2. Create input sequences (X) and corresponding output sequences (y)
def create_sequences(features, target, sequence_length):
    xs, ys = [], []
    for i in range(len(features) - sequence_length):
        # X sequence: 'sequence_length' time steps of feature data
        x_sequence = features.iloc[i:(i + sequence_length)].values
        # y value: target variable at the next time step following the X sequence
        y_value = target.iloc[i + sequence_length]
        xs.append(x_sequence)
        ys.append(y_value)
    return np.array(xs), np.array(ys)

X, y = create_sequences(features, target, sequence_length)

print(f"Shape of X (input sequences): {X.shape}")
print(f"Shape of y (output targets): {y.shape}")
print("First X sequence (first 5 values):\n", X[0,:,0][:5])
print("First y target:", y[0])

"""**Reasoning**:
Now that the sequential data (X and y) has been created, I will split it into training, validation, and testing sets while preserving temporal order, using 70% for training, 15% for validation, and 15% for testing, as specified in instruction 4 of the subtask. Finally, I will print the shapes of these sets to verify the data structure, as per instruction 5.


"""

import numpy as np

# 4. Split the data into training, validation, and testing sets
# Calculate split indices to maintain temporal order
total_samples = X.shape[0]
train_split = int(total_samples * 0.7)
val_split = int(total_samples * 0.85) # 70% train + 15% validation

X_train, y_train = X[:train_split], y[:train_split]
X_val, y_val = X[train_split:val_split], y[train_split:val_split]
X_test, y_test = X[val_split:], y[val_split:]

# 5. Print the shapes of the created sets
print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of X_val:   {X_val.shape}")
print(f"Shape of y_val:   {y_val.shape}")
print(f"Shape of X_test:  {X_test.shape}")
print(f"Shape of y_test:  {y_test.shape}")

"""**Reasoning**:
I will define and implement an LSTM model with Monte Carlo Dropout using TensorFlow and Keras, following the instructions to set up the architecture, include dropout layers, and compile the model for training.


"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input

# 1. Define model parameters based on data shapes
sequence_length = X_train.shape[1]  # 24
num_features = X_train.shape[2]     # 25
dropout_rate = 0.2

# 2. Define an LSTM model architecture with Monte Carlo Dropout
model = Sequential([
    Input(shape=(sequence_length, num_features)),
    LSTM(units=128, return_sequences=True),  # First LSTM layer, returns sequences for stacking
    Dropout(dropout_rate),                   # Dropout after first LSTM layer
    LSTM(units=64, return_sequences=False),  # Second LSTM layer, returns last output for Dense layer
    Dropout(dropout_rate),                   # Dropout after second LSTM layer
    Dense(units=1)                           # Output layer for regression
])

# 3. Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# 4. Print the model summary
print("LSTM Model with Monte Carlo Dropout Architecture:")
model.summary()

"""**Reasoning**:
Now that the LSTM model with Monte Carlo Dropout has been defined and compiled, I will train the model using the prepared training (`X_train`, `y_train`) and validation (`X_val`, `y_val`) datasets. I will specify the number of epochs and batch size for training, as well as use `validation_data` to monitor performance during training, aligning with instruction 7 of the subtask.


"""

epochs = 50
batch_size = 32

print(f"Training LSTM model for {epochs} epochs with a batch size of {batch_size}...")
history = model.fit(
    X_train, y_train,
    epochs=epochs,
    batch_size=batch_size,
    validation_data=(X_val, y_val),
    verbose=1
)

print("LSTM model training complete.")

import numpy as np
import tensorflow.keras.backend as K

# 1. Set the number of Monte Carlo samples
num_monte_carlo_samples = 100

# 2. Initialize an empty list to store predictions from each Monte Carlo run
mc_predictions = []

# 3. Loop `num_monte_carlo_samples` times to make predictions
print(f"Generating {num_monte_carlo_samples} Monte Carlo predictions...")
for i in range(num_monte_carlo_samples):
    # Ensure dropout layers are active during prediction by calling the model directly with training=True
    pred = model(X_test, training=True).numpy() # Call the model as a callable and convert to numpy
    mc_predictions.append(pred)

# 4. Convert the list of predictions into a NumPy array
mc_predictions = np.array(mc_predictions)
print(f"Shape of raw Monte Carlo predictions: {mc_predictions.shape}")

# 5. Calculate the mean prediction across all Monte Carlo samples
mean_prediction = np.mean(mc_predictions, axis=0)

# 6. Calculate the standard deviation of the predictions across all Monte Carlo samples
std_prediction = np.std(mc_predictions, axis=0)

# 7. Compute the 95% confidence interval lower bound (assuming a normal distribution)
lower_bound = mean_prediction - 1.96 * std_prediction

# 8. Compute the 95% confidence interval upper bound
upper_bound = mean_prediction + 1.96 * std_prediction

# 9. Print the shapes of the calculated results
print(f"\nShape of mean predictions: {mean_prediction.shape}")
print(f"Shape of standard deviation: {std_prediction.shape}")
print(f"Shape of 95% confidence interval lower bound: {lower_bound.shape}")
print(f"Shape of 95% confidence interval upper bound: {upper_bound.shape}")

print("Monte Carlo Dropout predictions and uncertainty intervals calculated.")

"""**Reasoning**:
I will calculate and report the Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Prediction Interval Coverage Probability (PICP), and Mean Prediction Interval Width (MPIW) for the LSTM model's predictions on the test set, as per the subtask instructions. I need to flatten `mean_prediction`, `lower_bound`, and `upper_bound` to ensure compatible shapes with `y_test` for calculations.


"""

from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

# Ensure y_test is a 1D array
y_test_flat = y_test.flatten()

# Ensure predictions and bounds are 1D arrays for metric calculation
mean_prediction_flat = mean_prediction.flatten()
lower_bound_flat = lower_bound.flatten()
upper_bound_flat = upper_bound.flatten()

# 1. Calculate MAE and RMSE for the LSTM model
mae_lstm = mean_absolute_error(y_test_flat, mean_prediction_flat)
rmse_lstm = np.sqrt(mean_squared_error(y_test_flat, mean_prediction_flat))

# 2. Compute Prediction Interval Coverage Probability (PICP)
# Count how many actual values fall within the 95% confidence interval
coverage = np.sum((y_test_flat >= lower_bound_flat) & (y_test_flat <= upper_bound_flat))
picp = (coverage / len(y_test_flat)) * 100

# 3. Calculate Mean Prediction Interval Width (MPIW)
mpiw = np.mean(upper_bound_flat - lower_bound_flat)

# 4. Print the calculated metrics
print("\nLSTM Model Performance on Test Set (Monte Carlo Dropout with 95% CI):")
print(f"Mean Absolute Error (MAE): {mae_lstm:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse_lstm:.4f}")
print(f"Prediction Interval Coverage Probability (PICP): {picp:.2f}%")
print(f"Mean Prediction Interval Width (MPIW): {mpiw:.4f}")

"""**Reasoning**:
I will calculate the Prediction Interval Coverage Probability (PICP) and Mean Prediction Interval Width (MPIW) for the Prophet model using the `performance_df` which contains the actual values and the Prophet model's prediction intervals, as per instructions 1 and 2.


"""

import numpy as np

# 1. Calculate Prediction Interval Coverage Probability (PICP) for Prophet
# Count how many actual values ('y') fall within the 80% confidence interval
prophet_coverage = np.sum(
    (performance_df['y'] >= performance_df['yhat_lower']) &
    (performance_df['y'] <= performance_df['yhat_upper'])
)
prophet_picp = (prophet_coverage / len(performance_df)) * 100

# 2. Calculate Mean Prediction Interval Width (MPIW) for Prophet
prophet_mpiw = np.mean(performance_df['yhat_upper'] - performance_df['yhat_lower'])

# 3. Print the calculated PICP and MPIW for the Prophet model
print("\nProphet Model Uncertainty Metrics:")
print(f"Prediction Interval Coverage Probability (PICP): {prophet_picp:.2f}%")
print(f"Mean Prediction Interval Width (MPIW): {prophet_mpiw:.4f}")

"""### Comparison of Prophet and LSTM Models

**Predictive Accuracy:**
*   **Prophet MAE**: 0.5518
*   **Prophet RMSE**: 0.6455
*   **LSTM MAE**: 0.1346
*   **LSTM RMSE**: 0.1869

**Uncertainty Quantification:**
*   **Prophet PICP**: 2.07%
*   **Prophet MPIW**: 0.0961
*   **LSTM PICP**: 98.15%
*   **LSTM MPIW**: 1.4145

#### Analysis:

**Predictive Accuracy (MAE, RMSE):**
The LSTM model significantly outperforms the Prophet model in terms of predictive accuracy. The LSTM model achieved a Mean Absolute Error (MAE) of 0.1346 and a Root Mean Squared Error (RMSE) of 0.1869, which are substantially lower than Prophet's MAE of 0.5518 and RMSE of 0.6455. This indicates that the LSTM model's point predictions are much closer to the actual values compared to Prophet.

**Uncertainty Quantification (PICP, MPIW):**
When it comes to uncertainty quantification, the LSTM model, leveraging Monte Carlo Dropout, demonstrates superior calibration. Its Prediction Interval Coverage Probability (PICP) is 98.15%, which is very close to the desired 95% confidence level, suggesting that its uncertainty intervals are well-calibrated and reliably capture the true values. In contrast, the Prophet model's PICP is a mere 2.07%, indicating that its 80% prediction intervals are severely under-covering the actual observations, meaning they are far too narrow or systematically misaligned with the true variability.

Regarding sharpness, the Prophet model has a much smaller Mean Prediction Interval Width (MPIW) of 0.0961 compared to LSTM's MPIW of 1.4145. While a smaller MPIW generally implies sharper (more precise) intervals, Prophet's sharpness comes at the severe cost of poor calibration (very low PICP). The LSTM model, despite having wider intervals, provides much more realistic and reliable uncertainty estimates due to its high PICP. A model with low PICP and small MPIW is typically considered poorly calibrated, as its narrow intervals fail to capture the true underlying distribution.

**Conclusion:**
The **LSTM model** is the superior choice for this time series forecasting task. It not only provides significantly more accurate point predictions (lower MAE and RMSE) but also offers well-calibrated uncertainty intervals (high PICP) through Monte Carlo Dropout, making its probabilistic forecasts much more reliable and useful than those from the Prophet model. While Prophet's intervals are 'sharper' (narrower), their extremely low coverage makes them practically unusable for uncertainty estimation.

### Comparison of Prophet and LSTM Models

**Predictive Accuracy:**
*   **Prophet MAE**: 0.5518
*   **Prophet RMSE**: 0.6455
*   **LSTM MAE**: 0.1346
*   **LSTM RMSE**: 0.1869

**Uncertainty Quantification:**
*   **Prophet PICP**: 2.07%
*   **Prophet MPIW**: 0.0961
*   **LSTM PICP**: 98.15%
*   **LSTM MPIW**: 1.4145

#### Analysis:

**Predictive Accuracy (MAE, RMSE):**
The LSTM model significantly outperforms the Prophet model in terms of predictive accuracy. The LSTM model achieved a Mean Absolute Error (MAE) of 0.1346 and a Root Mean Squared Error (RMSE) of 0.1869, which are substantially lower than Prophet's MAE of 0.5518 and RMSE of 0.6455. This indicates that the LSTM model's point predictions are much closer to the actual values compared to Prophet.

**Uncertainty Quantification (PICP, MPIW):**
When it comes to uncertainty quantification, the LSTM model, leveraging Monte Carlo Dropout, demonstrates superior calibration. Its Prediction Interval Coverage Probability (PICP) is 98.15%, which is very close to the desired 95% confidence level, suggesting that its uncertainty intervals are well-calibrated and reliably capture the true values. In contrast, the Prophet model's PICP is a mere 2.07%, indicating that its 80% prediction intervals are severely under-covering the actual observations, meaning they are far too narrow or systematically misaligned with the true variability.

Regarding sharpness, the Prophet model has a much smaller Mean Prediction Interval Width (MPIW) of 0.0961 compared to LSTM's MPIW of 1.4145. While a smaller MPIW generally implies sharper (more precise) intervals, Prophet's sharpness comes at the severe cost of poor calibration (very low PICP). The LSTM model, despite having wider intervals, provides much more realistic and reliable uncertainty estimates due to its high PICP. A model with low PICP and small MPIW is typically considered poorly calibrated, as its narrow intervals fail to capture the true underlying distribution.

**Conclusion:**
The **LSTM model** is the superior choice for this time series forecasting task. It not only provides significantly more accurate point predictions (lower MAE and RMSE) but also offers well-calibrated uncertainty intervals (high PICP) through Monte Carlo Dropout, making its probabilistic forecasts much more reliable and useful than those from the Prophet model. While Prophet's intervals are 'sharper' (narrower), their extremely low coverage makes them practically unusable for uncertainty estimation.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Create a DataFrame for LSTM results, aligning with the correct 'ds' values
# The 'y_test' corresponds to actual target values starting from df.iloc[val_split + sequence_length]

# Get the 'ds' values for the LSTM test set from the original prophet_df
lstm_ds_values = prophet_df['ds'].iloc[val_split + sequence_length : val_split + sequence_length + len(y_test)].reset_index(drop=True)

lstm_results_df = pd.DataFrame({
    'ds': lstm_ds_values,
    'actual_y': y_test_flat,
    'lstm_yhat': mean_prediction_flat,
    'lstm_yhat_lower': lower_bound_flat,
    'lstm_yhat_upper': upper_bound_flat
})

# 2. Align Prophet's performance_df to the same time period as LSTM's test set
# This ensures both models are compared on the exact same temporal test slice
comparison_df = pd.merge(
    lstm_results_df,
    performance_df[['ds', 'y', 'yhat', 'yhat_lower', 'yhat_upper']],
    on='ds',
    how='inner',
    suffixes=('_lstm', '_prophet')
)

# The 'y' column from performance_df is redundant with 'actual_y' from lstm_results_df
# Let's drop the redundant 'y' column.
comparison_df = comparison_df.drop(columns=['y'])

print("First 5 rows of the combined comparison DataFrame:")
print(comparison_df.head())
print(f"Shape of comparison DataFrame: {comparison_df.shape}")

# 3. Create the comparative visualizations
# Plot 1: Full test set comparison
plt.figure(figsize=(18, 9))
sns.lineplot(x='ds', y='actual_y', data=comparison_df, label='Actual Values', color='blue', linewidth=1)

sns.lineplot(x='ds', y='lstm_yhat', data=comparison_df, label='LSTM Forecast', color='red', linewidth=1)
plt.fill_between(comparison_df['ds'], comparison_df['lstm_yhat_lower'], comparison_df['lstm_yhat_upper'], color='red', alpha=0.1, label='LSTM 95% CI')

sns.lineplot(x='ds', y='yhat', data=comparison_df, label='Prophet Forecast', color='green', linewidth=1, linestyle='--')
plt.fill_between(comparison_df['ds'], comparison_df['yhat_lower'], comparison_df['yhat_upper'], color='green', alpha=0.1, label='Prophet 80% PI')

plt.title('Comparative Forecasts: Actual vs. LSTM vs. Prophet on Test Set')
plt.xlabel('Date')
plt.ylabel('Temperature (Scaled)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot 2: Zoomed-in section for better detail
plt.figure(figsize=(18, 9))
sns.lineplot(x='ds', y='actual_y', data=comparison_df.head(200), label='Actual Values', color='blue', linewidth=1)

sns.lineplot(x='ds', y='lstm_yhat', data=comparison_df.head(200), label='LSTM Forecast', color='red', linewidth=1)
plt.fill_between(comparison_df['ds'].head(200), comparison_df['lstm_yhat_lower'].head(200), comparison_df['lstm_yhat_upper'].head(200), color='red', alpha=0.1, label='LSTM 95% CI')

sns.lineplot(x='ds', y='yhat', data=comparison_df.head(200), label='Prophet Forecast', color='green', linewidth=1, linestyle='--')
plt.fill_between(comparison_df['ds'].head(200), comparison_df['yhat_lower'].head(200), comparison_df['yhat_upper'].head(200), color='green', alpha=0.1, label='Prophet 80% PI')

plt.title('Comparative Forecasts (Zoomed-in): Actual vs. LSTM vs. Prophet on Test Set')
plt.xlabel('Date')
plt.ylabel('Temperature (Scaled)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""## Summary:

### Data Analysis Key Findings

*   **LSTM Model Performance**: The LSTM model demonstrated strong predictive accuracy and well-calibrated uncertainty quantification. It achieved a Mean Absolute Error (MAE) of 0.1346 and a Root Mean Squared Error (RMSE) of 0.1869. Its 95% confidence intervals exhibited a Prediction Interval Coverage Probability (PICP) of 98.15% and a Mean Prediction Interval Width (MPIW) of 1.4145.
*   **Prophet Model Performance**: In contrast, the Prophet model showed significantly lower predictive accuracy and severely under-calibrated uncertainty intervals. Its MAE was 0.5518 and RMSE was 0.6455. Prophet's 80% prediction intervals had an extremely low PICP of 2.07%, despite a narrower MPIW of 0.0961.
*   **Comparative Predictive Accuracy**: The LSTM model significantly outperformed the Prophet model in point forecasting, achieving substantially lower MAE (0.1346 vs. 0.5518) and RMSE (0.1869 vs. 0.6455).
*   **Comparative Uncertainty Quantification**: The LSTM model, utilizing Monte Carlo Dropout, provided highly reliable uncertainty estimates with a PICP (98.15%) very close to the desired 95% confidence level. Prophet's prediction intervals, despite being sharper (smaller MPIW of 0.0961), were poorly calibrated with a mere 2.07% coverage, rendering them unreliable for capturing true variability.
*   **Visualization of Forecasts**: Comparative plots visually confirmed that the LSTM's forecasts aligned more closely with actual values and its uncertainty bands effectively enclosed the true observations, while Prophet's forecasts often missed the actual values, and its intervals were too narrow.

### Insights or Next Steps

*   **Model Suitability**: The LSTM model is clearly the superior choice for this time series forecasting task due to its significantly higher predictive accuracy and well-calibrated uncertainty quantification. Prophet's unreliability in uncertainty estimation makes it unsuitable when confidence intervals are critical.
*   **Further Optimization**: While the LSTM performed well, further investigation into hyperparameter tuning for Monte Carlo Dropout (e.g., dropout rate, number of samples) could potentially optimize the balance between coverage and sharpness of its uncertainty intervals.

"""

import os

# Create main results folder if it doesn't exist
os.makedirs('results/plots', exist_ok=True)

import pandas as pd

# Combine into DataFrame
intervals_df = lstm_results_df[['ds', 'lstm_yhat', 'lstm_yhat_lower', 'lstm_yhat_upper']].rename(
    columns={'ds': 'timestamp', 'lstm_yhat': 'point_forecast',
             'lstm_yhat_lower': 'lower_bound', 'lstm_yhat_upper': 'upper_bound'})

# Save as CSV
intervals_df.to_csv('results/intervals.csv', index=False)

mae = 0.1346
rmse = 0.1869
picp = 98.15
mpiw = 1.4145

with open('results/metrics.txt', 'w') as f:
    f.write(f"Model Performance Metrics:\n")
    f.write(f"Mean Absolute Error (MAE): {mae}\n")
    f.write(f"Root Mean Squared Error (RMSE): {rmse}\n")
    f.write(f"Prediction Interval Coverage Probability (PICP): {picp}%\n")
    f.write(f"Mean Prediction Interval Width (MPIW): {mpiw}\n")

import matplotlib.pyplot as plt

# Extract data for plotting from the intervals_df
timestamps = intervals_df['timestamp']
point_forecast = intervals_df['point_forecast']
lower_bound = intervals_df['lower_bound']
upper_bound = intervals_df['upper_bound']

# Example plotting code
plt.figure(figsize=(12,6))
plt.plot(timestamps, point_forecast, label='Point Forecast')
plt.fill_between(timestamps, lower_bound, upper_bound, color='red', alpha=0.3, label='Prediction Interval')
plt.legend()
plt.title('Forecast with Prediction Interval')
plt.xlabel('Time')
plt.ylabel('Value')

# Save the figure
plt.savefig('results/plots/forecast_with_intervals.png')
plt.close()  # Close plot to free memory